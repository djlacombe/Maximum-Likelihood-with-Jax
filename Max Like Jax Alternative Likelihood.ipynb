{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation Using Jax\n",
    "# Alternative Likelihood Function\n",
    "\n",
    "The following notebook is a brief introduction to [Jax](https://github.com/google/jax), which is a combination of [Autograd](https://github.com/hips/autograd) and [XLA](https://www.tensorflow.org/xla). \n",
    "\n",
    "In this example, we will be using automatic differentation to perform maximum likelihood estimation of the normal linear regression model.\n",
    "\n",
    "The inspiriation for this notebook comes from a blog post by [Rob Hicks](https://rlhick.people.wm.edu/posts/mle-autograd.html). We encourage interested readers to read the blog post first for an excellent introduction to the methodology and details of automatic diferentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first item is to import the required packages into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from scipy.optimize import minimize\n",
    "from jax.scipy import optimize \n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tke the data generation code from the blog post with these changes:\n",
    "\n",
    "```python\n",
    "K = 2\n",
    "beta = np.array([2,2])\n",
    "sigma = 0.5\n",
    "```\n",
    "The reason for this change is that we would like to know the true coefficients in the data generating process (DGP) so that we can verify that the optimization routine is correctly recovering the parameters of interest, i.e. we set each coefficient $\\beta$ to \"2\", change the number of explanatory variables to $K = 2$, and the error variance ($\\sigma = 0.5$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations\n",
    "N = 5000\n",
    "# number of parameters\n",
    "K = 2\n",
    "# true parameter values\n",
    "# beta = 2 * np.random.randn(K)\n",
    "beta = np.array([2.,2.])\n",
    "# true error std deviation\n",
    "sigma =  0.5\n",
    "\n",
    "def datagen(N, beta, sigma):\n",
    "    \"\"\"\n",
    "    Generates data for OLS regression.\n",
    "    Inputs:\n",
    "    N: Number of observations\n",
    "    beta: K x 1 true parameter values\n",
    "    sigma: std dev of error\n",
    "    \"\"\"\n",
    "    K = beta.shape[0]\n",
    "    x_ = 10 + 2 * np.random.randn(N,K-1)\n",
    "    # x is the N x K data matrix with column of ones\n",
    "    #   in the first position for estimating a constant\n",
    "    x = np.c_[np.ones(N),x_]\n",
    "    # y is the N x 1 vector of dependent variables\n",
    "    y = x.dot(beta) + sigma*np.random.randn(N)\n",
    "    return y, x\n",
    "\n",
    "y, x  = datagen(N, beta, sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code defines the negative log-likelihood function. We have altered the return of the likelihood function witht the following code:\n",
    "\n",
    "```python\n",
    "    return  (-1 * ll)/N\n",
    "```\n",
    "\n",
    "The reason for this change is that scaling the likelihood function can help with convergence of the optimization routine. The following [StackOverflow](https://stackoverflow.com/questions/24767191/scipy-is-not-optimizing-and-returns-desired-error-not-necessarily-achieved-due) post explains how this can help. Please read the second answer for additional details.\n",
    "\n",
    "Many numpy functions can be used in Jax by simply calling the folloiwng import:\n",
    "\n",
    "```python\n",
    "import jax.numpy as jnp\n",
    "```\n",
    "\n",
    "Most numpy functions can be called as usual with the addition of the alias \"jnp.\" in front of the name of the numpy function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_loglike(theta):\n",
    "    beta = theta[:-1]\n",
    "    sigma = theta[-1]\n",
    "    mu = jnp.dot(x,beta)\n",
    "    ll = -N/2 * jnp.log(2*jnp.pi*sigma**2) - (1/(2*sigma**2)) * jnp.sum((y - mu)**2)\n",
    "    return (-1 * ll)/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two lines of code caculate the Jacobian (jax.jacfwd) and Hessian (jax.hessian) matrices using automatic differentation. The Jacobian is calculated using forward mode automatic differentation. Additional details can be found at the Jax website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian = jax.jacfwd(neg_loglike)\n",
    "hessian = jax.hessian(neg_loglike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code appends the $\\beta$'s and $\\sigma$ to a vector called theta. Then, the Jacobian and Hessian is evaluated at these values. Note that some optimization algorithms don't use either a Jacobian or Hessian, some just the Jacobian, and some both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian : [ 0.0143818  0.1496062 -0.6774012] \n",
      "\n",
      "Hessian: [[2.0813713e+00 2.0689997e+01 4.1497082e-02]\n",
      " [2.0689997e+01 2.1384795e+02 4.3167233e-01]\n",
      " [4.1497108e-02 4.3167230e-01 1.2308879e+00]]\n"
     ]
    }
   ],
   "source": [
    "theta = jnp.append(beta,jnp.log(sigma))\n",
    "print(f'Jacobian : {jacobian(theta)} \\n')\n",
    "print(f'Hessian: {hessian(theta)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code utilizes scipy's minimize function to minimize our negative log-likelihood function. The method we use is BFGS and we add a tolerance option (i.e. 'gtol': 1e-7*N) according to the advice in the Stack Overflow post referenced above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.735323\n",
      "         Iterations: 22\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 77\n",
      "Convergence Achieved:  True\n",
      "Number of Function Evaluations:  77\n"
     ]
    }
   ],
   "source": [
    "theta_start = jax.numpy.append(jax.numpy.zeros(beta.shape[0]),0.5)\n",
    "res1 = minimize(neg_loglike, theta_start, method = 'BFGS', \n",
    "\t       options={'disp': True,'gtol': 1e-7*N}, jac = jacobian) # Tolerance added to aid in convergence\n",
    "print(\"Convergence Achieved: \", res1.success)\n",
    "print(\"Number of Function Evaluations: \", res1.nfev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the minimization routine is finished, you should see output with various metrics. The most important for our purposes is that the minimization algoritm has converged. We can see in the above output that Convergence Achieved is True, so we are ready to print out the results in the next code block.\n",
    "\n",
    "You'll notice a number of items printed out, with the last one being \"x\", which are the values of the coefficients (plus $\\sigma$ at the end) returned by the minimization routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: DeviceArray(0.73532254, dtype=float32)\n",
      " hess_inv: array([[ 6.92425916e+00, -6.64931847e-01, -5.98095563e-03],\n",
      "       [-6.64931847e-01,  6.65363398e-02,  1.23090512e-04],\n",
      "       [-5.98095563e-03,  1.23090512e-04,  1.27494148e-01]])\n",
      "      jac: array([-4.4964368e-06, -4.3905940e-05,  2.1484375e-06], dtype=float32)\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 77\n",
      "      nit: 22\n",
      "     njev: 77\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([2.00116366, 1.99918775, 0.5047887 ])\n"
     ]
    }
   ],
   "source": [
    "print(res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block prints out the coefficient estimates and exponentiates the $\\sigma$ parameter to place it in its original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient estimates: 2.00116366403242, 1.9991877528123372\n",
      "Original sigma: 0.5047886967598988\n"
     ]
    }
   ],
   "source": [
    "print(f'Coefficient estimates: {res1.x[0]}, {res1.x[1]}')\n",
    "\n",
    "print(f'Original sigma: {res1.x[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jax also has a built in minimization routine, which can be called with the next block of code. Note that the method option is restricted to BFGS at the time of writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = jax.scipy.optimize.minimize(neg_loglike, theta_start, tol=1e-7*N, method='BFGS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the results of this second minimization routine in the next block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizeResults(x=DeviceArray([2.0011656 , 1.9991876 , 0.50478846], dtype=float32), success=DeviceArray(False, dtype=bool), status=DeviceArray(3, dtype=int32, weak_type=True), fun=DeviceArray(0.7353226, dtype=float32), jac=DeviceArray([ 2.0379375e-06,  2.1465385e-05, -1.7920883e-09], dtype=float32), hess_inv=DeviceArray([[ 6.5450435 , -0.6321595 ,  0.03796447],\n",
      "             [-0.6321592 ,  0.06403862, -0.00670937],\n",
      "             [ 0.03796518, -0.00670943,  0.11934071]], dtype=float32), nfev=DeviceArray(69, dtype=int32, weak_type=True), njev=DeviceArray(69, dtype=int32, weak_type=True), nit=DeviceArray(33, dtype=int32, weak_type=True))\n"
     ]
    }
   ],
   "source": [
    "print(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the above output that the optimization routines has been successful (success=DeviceArray(True, dtype=bool)). We can print out the coefficients estimated and the value of the error variance i the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betas: 2.0011656284332275, 1.9991875886917114\n",
      "Original sigma: 0.5047886967598988\n"
     ]
    }
   ],
   "source": [
    "print(f'Betas: {res2.x[0]}, {res2.x[1]}')\n",
    "print(f'Original sigma: {res1.x[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like either minimization routine can be used to maximize the likelihood function. We would recommend the scipy.optimize.minimize function presently until Jax incorporates additional minimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope that this short tutorial shows the power of Jax and it's ease of use."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5bdc6b1a2c99e694589ab012f5ab618dc778716c4f263d9281f4a750b6806ff1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
